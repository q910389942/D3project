"""
Stable Diffusion 3 å›¾åƒç”Ÿæˆä¸ç›®æ ‡æ£€æµ‹è‡ªåŠ¨åŒ–æµæ°´çº¿

æœ¬è„šæœ¬çš„ä¸»è¦åŠŸèƒ½ï¼š
1. ä» COCO æˆ– VOC æ•°æ®é›†ä¸­åŠ è½½å›¾åƒæè¿°å’Œåœºæ™¯ä¿¡æ¯
2. ä½¿ç”¨ GPT æ¨¡å‹å¢å¼ºå›¾åƒæè¿°ï¼Œæ·»åŠ æ›´å¤šå¯¹è±¡ä¿¡æ¯
3. ä½¿ç”¨ Stable Diffusion 3 ç”Ÿæˆåˆæˆå›¾åƒ
4. ä½¿ç”¨ Faster R-CNN å¯¹ç”Ÿæˆçš„å›¾åƒè¿›è¡Œç›®æ ‡æ£€æµ‹
5. ç”Ÿæˆ YOLO æ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶å’Œå¯è§†åŒ–ç»“æœ

å·¥ä½œæµç¨‹ï¼š
æ•°æ®åŠ è½½ -> æç¤ºè¯å¢å¼º -> å›¾åƒç”Ÿæˆ -> ç›®æ ‡æ£€æµ‹ -> æ ‡æ³¨ä¿å­˜
"""

from pycocotools.coco import COCO

import torch,yaml,pickle
import os,json,random
from os.path import join
import argparse
import os
import matplotlib.pyplot as plt
from filelock import FileLock
from diffusers import StableDiffusion3Pipeline
from mmdet.apis import init_detector, inference_detector
import mmcv
import time
from itertools import chain
import logging  # æ·»åŠ ç¼ºå¤±çš„ logging æ¨¡å—
from PIL import Image  # ç”¨äºå›¾åƒæ‹¼æ¥


def str2bool(v):
    """
    å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå¸ƒå°”å€¼
    
    å‚æ•°:
        v: è¾“å…¥å€¼ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å¸ƒå°”å€¼ï¼‰
    
    è¿”å›:
        bool: è½¬æ¢åçš„å¸ƒå°”å€¼
    
    æ”¯æŒçš„å€¼:
        True: 'yes', 'true', 't', 'y', '1'
        False: 'no', 'false', 'f', 'n', '0'
    """
    if isinstance(v, bool):
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')

def gen(pipe, prompt, inferstep=50):
    """
    ä½¿ç”¨ Stable Diffusion 3 ç”Ÿæˆå›¾åƒï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼‰
    
    å‚æ•°:
        pipe: Stable Diffusion 3 çš„ç®¡é“å¯¹è±¡
        prompt: å›¾åƒç”Ÿæˆçš„æ–‡æœ¬æç¤ºè¯
        inferstep: æ¨ç†æ­¥æ•°ï¼Œé»˜è®¤50æ­¥ï¼ˆæ­¥æ•°è¶Šå¤šè´¨é‡è¶Šå¥½ä½†é€Ÿåº¦è¶Šæ…¢ï¼‰
    
    è¿”å›:
        PIL.Image: ç”Ÿæˆçš„å›¾åƒå¯¹è±¡
    """
    # ä½¿ç”¨å½“å‰æ—¶é—´æˆ³ç”Ÿæˆéšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡ç”Ÿæˆä¸åŒçš„å›¾åƒ
    # ä¿®æ”¹cudaä¸ºcpu/mpsï¼ˆMac M1 Pro ä½¿ç”¨ mpsï¼‰
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    generator = torch.Generator(device).manual_seed(int(time.time() * 1000) % 100000)
    # ä½¿ç”¨ç›¸åŒçš„æç¤ºè¯ä½œä¸º prompt å’Œ prompt_3ï¼ˆSD3 æ”¯æŒåŒæç¤ºè¯ï¼‰
    out = pipe(prompt, prompt_3=prompt, num_inference_steps=inferstep, generator=generator)
    return out.images[0]

def gen2(pipe, prompt, prompt3=None, inferstep=50):
    """
    ä½¿ç”¨ Stable Diffusion 3 ç”Ÿæˆå›¾åƒï¼ˆå¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒç‹¬ç«‹ prompt_3ï¼‰
    
    å‚æ•°:
        pipe: Stable Diffusion 3 çš„ç®¡é“å¯¹è±¡
        prompt: ä¸»è¦çš„å›¾åƒç”Ÿæˆæ–‡æœ¬æç¤ºè¯
        prompt3: å¯é€‰çš„ç¬¬ä¸‰ä¸ªæç¤ºè¯ï¼ˆSD3 ç‰¹æœ‰ï¼‰ï¼Œå¦‚æœä¸º None åˆ™åªä½¿ç”¨ prompt
        inferstep: æ¨ç†æ­¥æ•°ï¼Œé»˜è®¤50æ­¥
    
    è¿”å›:
        PIL.Image: ç”Ÿæˆçš„å›¾åƒå¯¹è±¡
    """
    # ä½¿ç”¨å½“å‰æ—¶é—´æˆ³ç”Ÿæˆéšæœºç§å­
    # ä¿®æ”¹cudaä¸ºcpu/mpsï¼ˆMac M1 Pro ä½¿ç”¨ mpsï¼‰
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    generator = torch.Generator(device).manual_seed(int(time.time() * 1000) % 100000)
    if prompt3 is None:
        # åªä½¿ç”¨å•ä¸ªæç¤ºè¯
        out = pipe(prompt, num_inference_steps=inferstep, generator=generator)
    else:
        # ä½¿ç”¨åŒæç¤ºè¯ï¼ˆprompt å’Œ prompt_3ï¼‰
        out = pipe(prompt, prompt_3=prompt3, num_inference_steps=inferstep, generator=generator)
    return out.images[0]

def gen_quadrant_image(pipe, prompt, inferstep=50, seed_offset=0):
    """
    ç”Ÿæˆå•ä¸ªè±¡é™çš„å›¾åƒï¼ˆ256x256ï¼‰
    
    å‚æ•°:
        pipe: Stable Diffusion 3 çš„ç®¡é“å¯¹è±¡
        prompt: è¯¥è±¡é™çš„å›¾åƒç”Ÿæˆæ–‡æœ¬æç¤ºè¯
        inferstep: æ¨ç†æ­¥æ•°
        seed_offset: ç§å­åç§»é‡ï¼Œç”¨äºç¡®ä¿ä¸åŒè±¡é™æœ‰ä¸åŒçš„éšæœºæ€§
    
    è¿”å›:
        PIL.Image: ç”Ÿæˆçš„256x256å›¾åƒå¯¹è±¡
    """
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    # ä½¿ç”¨æ—¶é—´æˆ³å’Œåç§»é‡ç”Ÿæˆä¸åŒçš„éšæœºç§å­
    generator = torch.Generator(device).manual_seed((int(time.time() * 1000) + seed_offset) % 100000)
    out = pipe(prompt, prompt_3=prompt, num_inference_steps=inferstep, generator=generator)
    # å°†ç”Ÿæˆçš„å›¾åƒè°ƒæ•´ä¸º256x256ï¼ˆæ¯ä¸ªè±¡é™çš„å¤§å°ï¼‰
    return out.images[0].resize((256, 256))

def combine_quadrants(top_left, top_right, bottom_left, bottom_right):
    """
    å°†4ä¸ªè±¡é™çš„å›¾åƒæ‹¼æ¥æˆä¸€å¼ å®Œæ•´çš„512x512å›¾åƒ
    
    å‚æ•°:
        top_left: å·¦ä¸Šè±¡é™å›¾åƒï¼ˆPIL.Imageï¼‰
        top_right: å³ä¸Šè±¡é™å›¾åƒï¼ˆPIL.Imageï¼‰
        bottom_left: å·¦ä¸‹è±¡é™å›¾åƒï¼ˆPIL.Imageï¼‰
        bottom_right: å³ä¸‹è±¡é™å›¾åƒï¼ˆPIL.Imageï¼‰
    
    è¿”å›:
        PIL.Image: æ‹¼æ¥åçš„512x512å®Œæ•´å›¾åƒ
    """
    # åˆ›å»ºä¸€å¼ 512x512çš„ç©ºç™½å›¾åƒ
    combined = Image.new('RGB', (512, 512))
    # å°†4ä¸ªè±¡é™çš„å›¾åƒç²˜è´´åˆ°å¯¹åº”ä½ç½®
    combined.paste(top_left, (0, 0))      # å·¦ä¸Šï¼šx=0, y=0
    combined.paste(top_right, (256, 0))   # å³ä¸Šï¼šx=256, y=0
    combined.paste(bottom_left, (0, 256)) # å·¦ä¸‹ï¼šx=0, y=256
    combined.paste(bottom_right, (256, 256)) # å³ä¸‹ï¼šx=256, y=256
    return combined

def parse_quadrant_captions(llm_output):
    """
    ä»LLMè¾“å‡ºä¸­è§£æ4ä¸ªè±¡é™çš„æè¿°
    
    å‚æ•°:
        llm_output: LLMè¿”å›çš„æ–‡æœ¬ï¼Œåº”åŒ…å«"Top Left:", "Top Right:", "Bottom Left:", "Bottom Right:"æ ‡è®°
    
    è¿”å›:
        dict: åŒ…å«4ä¸ªè±¡é™æè¿°çš„å­—å…¸ï¼Œé”®ä¸º'top_left', 'top_right', 'bottom_left', 'bottom_right'
    """
    import re
    quadrants = {}
    
    # æ¸…ç†è¾“å…¥æ–‡æœ¬ï¼šç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
    text = llm_output.strip()
    
    # å°è¯•æå–å„ä¸ªè±¡é™çš„æè¿°ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
    patterns = {
        'top_left': [
            r'Top Left:\s*(.+?)(?=Top Right:|Bottom Left:|Bottom Right:|Caption:|$)',
            r'top left:\s*(.+?)(?=top right:|bottom left:|bottom right:|$)',
            r'å·¦ä¸Š[ï¼š:]\s*(.+?)(?=å³ä¸Š|å·¦ä¸‹|å³ä¸‹|$)'
        ],
        'top_right': [
            r'Top Right:\s*(.+?)(?=Bottom Left:|Bottom Right:|$)',
            r'top right:\s*(.+?)(?=bottom left:|bottom right:|$)',
            r'å³ä¸Š[ï¼š:]\s*(.+?)(?=å·¦ä¸‹|å³ä¸‹|$)'
        ],
        'bottom_left': [
            r'Bottom Left:\s*(.+?)(?=Bottom Right:|$)',
            r'bottom left:\s*(.+?)(?=bottom right:|$)',
            r'å·¦ä¸‹[ï¼š:]\s*(.+?)(?=å³ä¸‹|$)'
        ],
        'bottom_right': [
            r'Bottom Right:\s*(.+?)(?=$)',
            r'bottom right:\s*(.+?)(?=$)',
            r'å³ä¸‹[ï¼š:]\s*(.+?)(?=$)'
        ]
    }
    
    # ä¸åŒºåˆ†å¤§å°å†™ï¼Œå…è®¸è·¨è¡ŒåŒ¹é…
    for key, pattern_list in patterns.items():
        found = False
        for pattern in pattern_list:
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                quadrants[key] = match.group(1).strip()
                # æ¸…ç†æè¿°ï¼šç§»é™¤å¤šä½™çš„æ¢è¡Œå’Œç©ºæ ¼
                quadrants[key] = ' '.join(quadrants[key].split())
                found = True
                break
        
        if not found:
            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨é»˜è®¤æè¿°
            quadrants[key] = "A scene with various objects."
            print(f"è­¦å‘Š: æ— æ³•è§£æ {key} è±¡é™çš„æè¿°ï¼Œä½¿ç”¨é»˜è®¤æè¿°")
    
    return quadrants


def get_llm_output(client, system_msg, user_prompt, model: str) -> str:
    """
    è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰API è·å–æ–‡æœ¬è¾“å‡º
    
    åŠŸèƒ½ï¼š
        ä½¿ç”¨ OpenAI API æˆ–å…¼å®¹çš„ API è°ƒç”¨ GPT æ¨¡å‹æ¥å¢å¼ºå›¾åƒæè¿°
        æ”¯æŒ GPT-3.5ã€GPT-4.1ã€GPT-5 å’Œ Vicuna æ¨¡å‹
    
    å‚æ•°:
        client: OpenAI å®¢æˆ·ç«¯å¯¹è±¡
        system_msg: ç³»ç»Ÿæç¤ºè¯ï¼Œå®šä¹‰æ¨¡å‹çš„è§’è‰²å’Œä»»åŠ¡
        user_prompt: ç”¨æˆ·æç¤ºè¯ï¼ŒåŒ…å«éœ€è¦å¤„ç†çš„å…·ä½“å†…å®¹
        model: æ¨¡å‹åç§°ï¼ˆå¦‚ 'gpt-4.1', 'gpt-3.5-turbo', 'vicuna'ï¼‰
    
    è¿”å›:
        str: LLM ç”Ÿæˆçš„æ–‡æœ¬å“åº”
    
    å¼‚å¸¸:
        ValueError: å¦‚æœ API è°ƒç”¨å¤±è´¥
    """
    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆç”¨äº Chat æ¨¡å‹ï¼‰
    if model in ["gpt-3.5", "gpt-3.5-turbo", "gpt-4.1", 'gpt-5']:
        messages = [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt},
        ]
    else:
        # å¯¹äºé Chat æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨ prompt
        messages = user_prompt
    
    # ç”Ÿæˆç¼“å­˜é”®ï¼ˆå¯ç”¨äºåç»­çš„ç¼“å­˜åŠŸèƒ½ï¼‰
    key = json.dumps([model, messages])
    
    # å°è¯•è°ƒç”¨ APIï¼ˆç›®å‰åªå°è¯•ä¸€æ¬¡ï¼‰
    for _ in range(1):
        try:
            if model in ["gpt-3.5", "gpt-3.5-turbo", "gpt-4.1", 'gpt-5']:
                # ä½¿ç”¨ Chat Completions API
                completion = client.chat.completions.create(
                    model=model,
                    messages=messages,
                )
                response = completion.choices[0].message.content
            elif model == "vicuna":
                # ä½¿ç”¨ Completions APIï¼ˆé€‚ç”¨äº Vicuna ç­‰æ¨¡å‹ï¼‰
                completion = client.completions.create(
                    model="lmsys/vicuna-7b-v1.5",
                    prompt=user_prompt,  # Vicuna ä½¿ç”¨ prompt è€Œä¸æ˜¯ messages
                    max_tokens=512,
                    temperature=0,  # æ¸©åº¦è®¾ä¸º0ï¼Œä½¿ç”¨è´ªå©ªè§£ç 
                )
                response = completion.choices[0].text
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ç¼“å­˜ä¿å­˜åŠŸèƒ½
            # save_to_cache(key, response, llm_cache)
            return response

        except Exception as e:
            # è®°å½•é”™è¯¯å¹¶ç»§ç»­å°è¯•
            logging.error(f"LLM Error: {e}")
            continue
    
    # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
    raise ValueError("Failed to get LLM output after retries")


def get_cap():
    """
    ä»æ•°æ®é›†ä¸­åŠ è½½å›¾åƒæè¿°ï¼ˆcaptionï¼‰å’Œåœºæ™¯-å¯¹è±¡æ˜ å°„å…³ç³»
    
    åŠŸèƒ½ï¼š
        1. æ ¹æ®æ•°æ®é›†ç±»å‹ï¼ˆCOCO æˆ– VOCï¼‰åŠ è½½ç›¸åº”çš„æ•°æ®
        2. æå–æ¯å¼ å›¾åƒçš„æ–‡æœ¬æè¿°å’ŒåŒ…å«çš„å¯¹è±¡ç±»åˆ«
        3. æ„å»ºåœºæ™¯åˆ°å¯¹è±¡çš„æ˜ å°„å­—å…¸ï¼ˆscene2objï¼‰ï¼Œç”¨äºåç»­çš„å¯¹è±¡å¢å¼º
    
    è¿”å›:
        tuple: (all_captions, scene2obj)
            - all_captions: åŒ…å«æ‰€æœ‰å›¾åƒæè¿°ä¿¡æ¯çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
                - 'text': å›¾åƒçš„æ–‡å­—æè¿°
                - 'scene': åœºæ™¯ç±»åˆ«
                - 'classes': å›¾åƒä¸­åŒ…å«çš„å¯¹è±¡ç±»åˆ«åˆ—è¡¨
            - scene2obj: å­—å…¸ï¼Œé”®ä¸ºåœºæ™¯ç±»åˆ«ï¼Œå€¼ä¸ºè¯¥åœºæ™¯ä¸­å¯èƒ½å‡ºç°çš„å¯¹è±¡åˆ—è¡¨
    
    æ•°æ®é›†æ”¯æŒ:
        - COCO: ä½¿ç”¨ COCO æ•°æ®é›†çš„æ ‡æ³¨æ–‡ä»¶
        - VOC: ä½¿ç”¨ VOC æ•°æ®é›†çš„ BLIP æ ‡æ³¨æ–‡ä»¶
    """
    if args.dataset == 'coco':
        # ========== COCO æ•°æ®é›†å¤„ç† ==========
        # åŠ è½½ COCO å®ä¾‹æ ‡æ³¨æ–‡ä»¶ï¼ˆåŒ…å«å¯¹è±¡æ£€æµ‹ä¿¡æ¯ï¼‰
        with open("/local_dataset/coco_tamlt/annotations/instances_train2017.json", "r") as f:
                coco_data = json.load(f)
        
        # åˆ›å»ºç±»åˆ«IDåˆ°ç±»åˆ«åç§°çš„æ˜ å°„å­—å…¸
        category_map = {category['id']: category['name'] for category in coco_data['categories']}
        
        # åŠ è½½ YOLO æ ¼å¼çš„ç±»åˆ«åç§°æ–‡ä»¶
        with open('/NAS2/tamlt/tamlt/Code/LIC/YOLO/data/coco.yaml', 'r') as file:
            names = yaml.safe_load(file)['names']
        reverse_names = {name: class_id for class_id, name in names.items()}
        
        # åˆå§‹åŒ– COCO API å¯¹è±¡
        coco = COCO("/local_dataset/coco_tamlt/annotations/instances_train2017.json")  # å¯¹è±¡æ£€æµ‹æ ‡æ³¨
        coco_caps = COCO("/local_dataset/coco_tamlt/annotations/captions_train2017.json")  # å›¾åƒæè¿°æ ‡æ³¨
        
        print('Here')
        # è·å–æ‰€æœ‰å›¾åƒIDÂµ
        original_id = coco.getImgIds()

        all_captions = []

        # åŠ è½½é¢„å¤„ç†çš„åœºæ™¯åˆ†ç±»ç»“æœï¼ˆæ¯å¼ å›¾åƒå¯¹åº”çš„åœºæ™¯ç±»åˆ«ï¼‰
        allid = torch.load('/NAS2/tamlt/tamlt/Code/LIC/DCOD/results/characteristic/coco_all')
        # Places365 åœºæ™¯ç±»åˆ«çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ¯ä¸ªåœºæ™¯åœ¨æ•°æ®é›†ä¸­çš„å‡ºç°æ¬¡æ•°ï¼‰
        # è¿™ä¸ªå­—å…¸å®šä¹‰äº†æ‰€æœ‰å¯èƒ½çš„åœºæ™¯ç±»åˆ«
        probs = {'airfield': 157, 'airplane_cabin': 95, 'airport_terminal': 20, 'alcove': 1183, 'alley': 35, 'amphitheater': 8, 'amusement_arcade': 5, 'amusement_park': 380, 'apartment_building/outdoor': 13, 'aquarium': 202, 'aqueduct': 24, 'arcade': 13, 'arch': 54, 'archaelogical_excavation': 6, 'archive': 935, 'arena/hockey': 16, 'arena/performance': 18, 'arena/rodeo': 1085, 'army_base': 5, 'art_gallery': 1323, 'art_school': 0, 'art_studio': 317, 'artists_loft': 15, 'assembly_line': 29, 'athletic_field/outdoor': 156, 'atrium/public': 3, 'attic': 315, 'auditorium': 6, 'auto_factory': 58, 'auto_showroom': 5, 'badlands': 109, 'bakery/shop': 645, 'balcony/exterior': 4, 'balcony/interior': 23, 'ball_pit': 299, 'ballroom': 60, 'bamboo_forest': 8, 'bank_vault': 37, 'banquet_hall': 190, 'bar': 62, 'barn': 45, 'barndoor': 88, 'baseball_field': 2652, 'basement': 671, 'basketball_court/indoor': 76, 'bathroom': 4372, 'bazaar/indoor': 0, 'bazaar/outdoor': 51, 'beach': 1292, 'beach_house': 9, 'beauty_salon': 149, 'bedchamber': 17, 'bedroom': 758, 'beer_garden': 2, 'beer_hall': 1, 'berth': 194, 'biology_laboratory': 6, 'boardwalk': 74, 'boat_deck': 283, 'boathouse': 61, 'bookstore': 23, 'booth/indoor': 15, 'botanical_garden': 13, 'bow_window/indoor': 167, 'bowling_alley': 11, 'boxing_ring': 95, 'bridge': 398, 'building_facade': 26, 'bullring': 174, 'burial_chamber': 1830, 'bus_interior': 12, 'bus_station/indoor': 820, 'butchers_shop': 34, 'butte': 11, 'cabin/outdoor': 0, 'cafeteria': 128, 'campsite': 127, 'campus': 14, 'canal/natural': 35, 'canal/urban': 13, 'candy_store': 227, 'canyon': 0, 'car_interior': 28, 'carrousel': 10, 'castle': 8, 'catacomb': 26, 'cemetery': 646, 'chalet': 7, 'chemistry_lab': 1190, 'childs_room': 908, 'church/indoor': 27, 'church/outdoor': 28, 'classroom': 387, 'clean_room': 4503, 'cliff': 16, 'closet': 2238, 'clothing_store': 21, 'coast': 82, 'cockpit': 9, 'coffee_shop': 472, 'computer_room': 304, 'conference_center': 44, 'conference_room': 34, 'construction_site': 393, 'corn_field': 32, 'corral': 310, 'corridor': 53, 'cottage': 0, 'courthouse': 109, 'courtyard': 10, 'creek': 1, 'crevasse': 19, 'crosswalk': 429, 'dam': 35, 'delicatessen': 32, 'department_store': 11, 'desert/sand': 164, 'desert/vegetation': 6840, 'desert_road': 53, 'diner/outdoor': 0, 'dining_hall': 40, 'dining_room': 199, 'discotheque': 1748, 'doorway/outdoor': 13, 'dorm_room': 90, 'downtown': 87, 'dressing_room': 940, 'driveway': 199, 'drugstore': 404, 'elevator/door': 229, 'elevator_lobby': 0, 'elevator_shaft': 5, 'embassy': 62, 'engine_room': 157, 'entrance_hall': 10, 'escalator/indoor': 3, 'excavation': 27, 'fabric_store': 24, 'farm': 23, 'fastfood_restaurant': 0, 'field/cultivated': 142, 'field/wild': 62, 'field_road': 866, 'fire_escape': 198, 'fire_station': 748, 'fishpond': 196, 'flea_market/indoor': 0, 'florist_shop/indoor': 496, 'food_court': 20, 'football_field': 19, 'forest/broadleaf': 460, 'forest_path': 82, 'forest_road': 89, 'formal_garden': 0, 'fountain': 55, 'galley': 2, 'garage/indoor': 34, 'garage/outdoor': 1, 'gas_station': 198, 'gazebo/exterior': 16, 'general_store/indoor': 0, 'general_store/outdoor': 4, 'gift_shop': 12, 'glacier': 94, 'golf_course': 311, 'greenhouse/indoor': 20, 'greenhouse/outdoor': 8, 'grotto': 0, 'gymnasium/indoor': 2, 'hangar/indoor': 25, 'hangar/outdoor': 127, 'harbor': 785, 'hardware_store': 14, 'hayfield': 632, 'heliport': 142, 'highway': 769, 'home_office': 118, 'home_theater': 22, 'hospital': 42, 'hospital_room': 460, 'hot_spring': 2315, 'hotel/outdoor': 3, 'hotel_room': 305, 'house': 1, 'hunting_lodge/outdoor': 0, 'ice_cream_parlor': 1745, 'ice_floe': 2535, 'ice_shelf': 1601, 'ice_skating_rink/indoor': 1010, 'ice_skating_rink/outdoor': 657, 'iceberg': 536, 'igloo': 6991, 'industrial_area': 454, 'inn/outdoor': 2, 'islet': 45, 'jacuzzi/indoor': 224, 'jail_cell': 148, 'japanese_garden': 11, 'jewelry_shop': 127, 'junkyard': 85, 'kasbah': 90, 'kennel/outdoor': 209, 'kindergarden_classroom': 115, 'kitchen': 208, 'lagoon': 6, 'lake/natural': 194, 'landfill': 35, 'landing_deck': 1, 'laundromat': 61, 'lawn': 10, 'lecture_room': 60, 'legislative_chamber': 1, 'library/indoor': 18, 'library/outdoor': 3, 'lighthouse': 183, 'living_room': 695, 'loading_dock': 68, 'lobby': 2, 'lock_chamber': 13, 'locker_room': 244, 'mansion': 6, 'manufactured_home': 1, 'market/indoor': 0, 'market/outdoor': 0, 'marsh': 32, 'martial_arts_gym': 574, 'mausoleum': 36, 'medina': 69, 'mezzanine': 10, 'moat/water': 22, 'mosque/outdoor': 846, 'motel': 1790, 'mountain': 31, 'mountain_path': 19, 'mountain_snowy': 94, 'movie_theater/indoor': 2, 'museum/indoor': 4357, 'museum/outdoor': 1, 'music_studio': 109, 'natural_history_museum': 1368, 'nursery': 4453, 'nursing_home': 77, 'oast_house': 40, 'ocean': 386, 'office': 828, 'office_building': 27, 'office_cubicles': 69, 'oilrig': 234, 'operating_room': 22, 'orchard': 146, 'orchestra_pit': 0, 'pagoda': 111, 'palace': 1, 'pantry': 319, 'park': 87, 'parking_garage/indoor': 12, 'parking_garage/outdoor': 2, 'parking_lot': 738, 'pasture': 1015, 'patio': 172, 'pavilion': 13, 'pet_shop': 13, 'pharmacy': 325, 'phone_booth': 51, 'physics_laboratory': 2, 'picnic_area': 99, 'pier': 87, 'pizzeria': 250, 'playground': 797, 'playroom': 1227, 'plaza': 26, 'pond': 227, 'porch': 101, 'promenade': 186, 'pub/indoor': 0, 'racecourse': 659, 'raceway': 2390, 'raft': 49, 'railroad_track': 1176, 'rainforest': 68, 'reception': 7, 'recreation_room': 8, 'repair_shop': 31, 'residential_neighborhood': 4, 'restaurant': 57, 'restaurant_kitchen': 11, 'restaurant_patio': 0, 'rice_paddy': 243, 'river': 7, 'rock_arch': 3, 'roof_garden': 0, 'rope_bridge': 62, 'ruin': 1, 'runway': 1739, 'sandbox': 115, 'sauna': 1150, 'schoolhouse': 317, 'science_museum': 337, 'server_room': 38, 'shed': 34, 'shoe_shop': 66, 'shopfront': 8, 'shopping_mall/indoor': 0, 'shower': 1586, 'ski_resort': 18, 'ski_slope': 950, 'sky': 766, 'skyscraper': 129, 'slum': 436, 'snowfield': 301, 'soccer_field': 368, 'stable': 252, 'stadium/baseball': 176, 'stadium/football': 16, 'stadium/soccer': 59, 'stage/indoor': 294, 'stage/outdoor': 0, 'staircase': 223, 'storage_room': 19, 'street': 276, 'subway_station/platform': 207, 'supermarket': 47, 'sushi_bar': 0, 'swamp': 12, 'swimming_hole': 19, 'swimming_pool/indoor': 20, 'swimming_pool/outdoor': 1, 'synagogue/outdoor': 58, 'television_room': 4, 'television_studio': 1, 'temple/asia': 24, 'throne_room': 9, 'ticket_booth': 111, 'topiary_garden': 7, 'tower': 518, 'toyshop': 28, 'train_interior': 5, 'train_station/platform': 595, 'tree_farm': 572, 'tree_house': 21, 'trench': 216, 'tundra': 197, 'underwater/ocean_deep': 178, 'utility_room': 587, 'valley': 41, 'vegetable_garden': 7, 'veterinarians_office': 203, 'viaduct': 75, 'village': 6, 'vineyard': 11, 'volcano': 25, 'volleyball_court/outdoor': 127, 'waiting_room': 54, 'water_park': 3, 'water_tower': 307, 'waterfall': 0, 'watering_hole': 1050, 'wave': 750, 'wet_bar': 282, 'wheat_field': 275, 'wind_farm': 251, 'windmill': 285, 'yard': 147, 'youth_hostel': 69, 'zen_garden': 10}
        
        # è·å–æ‰€æœ‰åœºæ™¯ç±»åˆ«åˆ—è¡¨
        scene_all = list(probs.keys())
        # åˆå§‹åŒ–åœºæ™¯åˆ°å¯¹è±¡çš„æ˜ å°„å­—å…¸ï¼ˆæ¯ä¸ªåœºæ™¯å¯¹åº”ä¸€ä¸ªå¯¹è±¡åˆ—è¡¨ï¼‰
        scene2obj = {_: [] for _ in scene_all}
        
        # éå†æ‰€æœ‰å›¾åƒï¼Œæå–æè¿°å’Œå¯¹è±¡ä¿¡æ¯
        for i, image_id in enumerate(original_id):
            # è·å–è¯¥å›¾åƒçš„æ‰€æœ‰æè¿°æ ‡æ³¨ID
            caption_ids = coco_caps.getAnnIds(imgIds=image_id)
            # åŠ è½½æè¿°æ ‡æ³¨ï¼ˆé€šå¸¸æ¯å¼ å›¾åƒæœ‰å¤šä¸ªæè¿°ï¼‰
            captions = coco_caps.loadAnns(caption_ids)
            
            # è·å–è¯¥å›¾åƒçš„æ‰€æœ‰å¯¹è±¡æ£€æµ‹æ ‡æ³¨ID
            ann_ids = coco.getAnnIds(imgIds=image_id)
            # åŠ è½½å¯¹è±¡æ£€æµ‹æ ‡æ³¨
            anns = coco.loadAnns(ann_ids)
            
            # æå–å›¾åƒä¸­æ‰€æœ‰å¯¹è±¡çš„ç±»åˆ«åç§°
            classes = []
            for ann in anns:
                coco_category_id = ann["category_id"]
                coco_class_name = category_map.get(coco_category_id)
                classes.append(coco_class_name)
            
            # æ„å»ºè¯¥å›¾åƒçš„æ•°æ®å­—å…¸
            data = {
                'text': captions[0]['caption'],  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæè¿°ä½œä¸ºæ–‡æœ¬
                'scene': allid[image_id],  # ä»é¢„åŠ è½½çš„åœºæ™¯åˆ†ç±»ç»“æœä¸­è·å–åœºæ™¯ç±»åˆ«
                'classes': list(set(classes))  # å»é‡åçš„å¯¹è±¡ç±»åˆ«åˆ—è¡¨
            }
            
            # æ›´æ–°åœºæ™¯åˆ°å¯¹è±¡çš„æ˜ å°„ï¼šå°†è¯¥å›¾åƒä¸­çš„å¯¹è±¡æ·»åŠ åˆ°å¯¹åº”åœºæ™¯çš„å¯¹è±¡åˆ—è¡¨ä¸­
            v = list(set(scene2obj[allid[image_id]] + list(classes)))
            scene2obj[allid[image_id]] = v
            
            # æ·»åŠ åˆ°æ€»åˆ—è¡¨
            all_captions.append(data)
    elif args.dataset == 'voc':
        # ========== VOC æ•°æ®é›†å¤„ç† ==========
        categories_file = './results/categories_places365.txt'
        voc_blip_dir = './results/voc_blip'
        characteristic_file = './results/characteristic/voc_all'
        print("ğŸ“ ä½¿ç”¨ results/ æ•°æ®é›†")
        
        # åŠ è½½ Places365 åœºæ™¯ç±»åˆ«æ–‡ä»¶
        with open(categories_file) as f:
            # ä»æ¯è¡Œä¸­æå–åœºæ™¯ç±»åˆ«åç§°ï¼ˆè·³è¿‡å‰3ä¸ªå­—ç¬¦ï¼Œå¦‚ "/a/airfield" -> "airfield"ï¼‰
            categories = [line.strip().split(' ')[0][3:] for line in f.readlines()]
        
        # åˆå§‹åŒ–åœºæ™¯åˆ°å¯¹è±¡çš„æ˜ å°„å­—å…¸
        scene2obj = {_: [] for _ in categories}
        
        # VOC æ•°æ®é›†çš„ BLIP æ ‡æ³¨æ–‡ä»¶ç›®å½•
        p = voc_blip_dir
        # è·å–æ‰€æœ‰æ ‡æ³¨æ–‡ä»¶
        files = [f for f in os.listdir(p) if os.path.isfile(os.path.join(p, f))]
        all_captions = []
        
        # åŠ è½½é¢„å¤„ç†çš„åœºæ™¯åˆ†ç±»ç»“æœï¼ˆå›¾åƒæ–‡ä»¶ååˆ°åœºæ™¯ç±»åˆ«çš„æ˜ å°„ï¼‰
        allid = torch.load(characteristic_file)
        
        # éå†æ‰€æœ‰æ ‡æ³¨æ–‡ä»¶
        for fi in files:
            p2 = join(p, fi)
            with open(p2, 'rb') as f:
                # ä»æ–‡ä»¶åä¸­æå–å›¾åƒæ ‡è¯†ï¼ˆå»æ‰å‰ç¼€ï¼‰
                ff = "_".join(fi.split("_")[1:])
                # è·å–è¯¥å›¾åƒå¯¹åº”çš„åœºæ™¯ç±»åˆ«
                xx = allid[ff]
                
                # åŠ è½½ pickle æ ¼å¼çš„æ ‡æ³¨æ•°æ®ï¼ˆåŒ…å«æè¿°å’Œå¯¹è±¡ä¿¡æ¯ï¼‰
                x = pickle.load(f)
                
                # æ„å»ºæ•°æ®å­—å…¸
                d = {
                    'text': x['cap'],  # BLIP ç”Ÿæˆçš„å›¾åƒæè¿°
                    'classes': x['obj'],  # å›¾åƒä¸­çš„å¯¹è±¡ç±»åˆ«åˆ—è¡¨
                    'scene': xx  # åœºæ™¯ç±»åˆ«
                     }
                
                # æ›´æ–°åœºæ™¯åˆ°å¯¹è±¡çš„æ˜ å°„
                v = list(set(scene2obj[xx] + list(x['obj'])))
                scene2obj[xx] = v
                
                # æ·»åŠ åˆ°æ€»åˆ—è¡¨
                all_captions.append(d)
                

    else:
        raise ValueError
    
    return all_captions,scene2obj

def save_predict(img_path, results, fn, output_img_dir, label_gen_path):
    """
    ä¿å­˜ç›®æ ‡æ£€æµ‹ç»“æœï¼šç”Ÿæˆå¯è§†åŒ–å›¾åƒå’Œ YOLO æ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶
    
    åŠŸèƒ½ï¼š
        1. å¯¹æ£€æµ‹ç»“æœåº”ç”¨å¤šä¸ªç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆ0.3-0.9ï¼‰
        2. ä¸ºæ¯ä¸ªé˜ˆå€¼ç”Ÿæˆå¸¦æ£€æµ‹æ¡†çš„å¯è§†åŒ–å›¾åƒ
        3. ç”Ÿæˆ YOLO æ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶ï¼ˆç±»åˆ«ID + å½’ä¸€åŒ–çš„è¾¹ç•Œæ¡†åæ ‡ï¼‰
    
    å‚æ•°:
        img_path: è¾“å…¥å›¾åƒè·¯å¾„
        results: MMDetection çš„æ£€æµ‹ç»“æœå¯¹è±¡
        fn: æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        output_img_dir: è¾“å‡ºå›¾åƒç›®å½•
        label_gen_path: è¾“å‡ºæ ‡æ³¨æ–‡ä»¶ç›®å½•
    
    è¾“å‡ºæ–‡ä»¶æ ¼å¼:
        - å¯è§†åŒ–å›¾åƒ: {output_dir}/t-{threshold}/{fn}-PRED.jpg
        - æ ‡æ³¨æ–‡ä»¶: {label_dir}/t-{threshold}/{fn}-GEN.txt
          æ ‡æ³¨æ ¼å¼: "class_id x_center y_center width height" (æ‰€æœ‰å€¼å½’ä¸€åŒ–åˆ° [0,1])
    """
    # å®šä¹‰å¤šä¸ªç½®ä¿¡åº¦é˜ˆå€¼ï¼Œç”¨äºç”Ÿæˆä¸åŒä¸¥æ ¼ç¨‹åº¦çš„æ£€æµ‹ç»“æœ
    target_conf = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3]
    
    # å¯¹æ¯ä¸ªç½®ä¿¡åº¦é˜ˆå€¼åˆ†åˆ«å¤„ç†
    for tc in target_conf:
        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼šå¯è§†åŒ–å›¾åƒç›®å½•å’Œæ ‡æ³¨æ–‡ä»¶ç›®å½•
        odir = join(output_img_dir, 't-{}'.format(tc))  # å¯è§†åŒ–å›¾åƒç›®å½•
        lbdir = join(label_gen_path, 't-{}'.format(tc))  # æ ‡æ³¨æ–‡ä»¶ç›®å½•
        os.makedirs(odir, exist_ok=True)
        os.makedirs(lbdir, exist_ok=True)

        # è¯»å–å›¾åƒ
        img = mmcv.imread(img_path)
        # åˆ›å»º matplotlib å›¾å½¢ç”¨äºç»˜åˆ¶æ£€æµ‹æ¡†
        plt.figure(figsize=(8, 8))
        plt.imshow(mmcv.bgr2rgb(img))  # è½¬æ¢ä¸º RGB æ ¼å¼æ˜¾ç¤º
        plt.axis('off')

        # å­˜å‚¨æ ‡æ³¨æ–‡æœ¬ï¼ˆYOLO æ ¼å¼ï¼‰
        msg = []
        
        # ä»æ£€æµ‹ç»“æœä¸­æå–ä¿¡æ¯
        instances = results.pred_instances  # InstanceData å¯¹è±¡
        boxes = instances.bboxes.cpu().numpy()  # è¾¹ç•Œæ¡†åæ ‡ï¼Œå½¢çŠ¶ (N, 4)ï¼Œæ ¼å¼ [x1, y1, x2, y2]
        scores = instances.scores.cpu().numpy()  # ç½®ä¿¡åº¦åˆ†æ•°ï¼Œå½¢çŠ¶ (N,)
        labels = instances.labels.cpu().numpy()  # ç±»åˆ«IDï¼Œå½¢çŠ¶ (N,)
        
        # éå†æ‰€æœ‰æ£€æµ‹ç»“æœ
        for box, conf, cls_id in zip(boxes, scores, labels):
                x1, y1, x2, y2 = box
            
            # å¦‚æœç½®ä¿¡åº¦ä½äºå½“å‰é˜ˆå€¼ï¼Œè·³è¿‡è¯¥æ£€æµ‹
            if conf < tc:
                    continue

            # åœ¨å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹æ¡†ï¼ˆçº¢è‰²è¾¹æ¡†ï¼‰
                plt.gca().add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1,
                                                edgecolor='red', facecolor='none', linewidth=2))
            # åœ¨æ£€æµ‹æ¡†å·¦ä¸Šè§’æ·»åŠ ç±»åˆ«IDå’Œç½®ä¿¡åº¦æ–‡æœ¬
                plt.text(x1, y1, f'{cls_id}: {conf:.2f}', color='white',
                        bbox=dict(facecolor='red', edgecolor='none', pad=1))

            # è½¬æ¢ä¸º YOLO æ ¼å¼ï¼šè®¡ç®—å½’ä¸€åŒ–çš„ä¸­å¿ƒç‚¹åæ ‡å’Œå®½é«˜
            x_center = (x1 + x2) / 2.0  # è¾¹ç•Œæ¡†ä¸­å¿ƒ x åæ ‡
            y_center = (y1 + y2) / 2.0  # è¾¹ç•Œæ¡†ä¸­å¿ƒ y åæ ‡
            obj_w = x2 - x1  # è¾¹ç•Œæ¡†å®½åº¦
            obj_h = y2 - y1  # è¾¹ç•Œæ¡†é«˜åº¦

            # å½’ä¸€åŒ–åˆ°å›¾åƒå°ºå¯¸ï¼ˆYOLO æ ¼å¼è¦æ±‚æ‰€æœ‰åæ ‡åœ¨ [0, 1] èŒƒå›´å†…ï¼‰
                h, w = img.shape[:2]
            # YOLO æ ¼å¼ï¼šclass_id x_center y_center width heightï¼ˆå…¨éƒ¨å½’ä¸€åŒ–ï¼‰
                m = f'{cls_id} {x_center / w:.6f} {y_center / h:.6f} {obj_w / w:.6f} {obj_h / h:.6f}'
                msg.append(m)
            
        # ä¿å­˜å¯è§†åŒ–å›¾åƒ
        plt.axis('off')
        ff = '{}/{}-PRED.jpg'.format(odir, fn)
        plt.savefig(ff)
        plt.close()

        # ä¿å­˜ YOLO æ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªæ£€æµ‹æ¡†ï¼‰
        label_f = open(join(lbdir, '{}-GEN.txt'.format(fn)), 'w')
        label_f.write('\n'.join(msg))
        label_f.close()


def gen_04():
    """
    ä¸»å‡½æ•°ï¼šå®Œæ•´çš„å›¾åƒç”Ÿæˆä¸æ£€æµ‹æµæ°´çº¿
    
    å·¥ä½œæµç¨‹ï¼š
        1. åˆå§‹åŒ– OpenAI API å®¢æˆ·ç«¯
        2. æ ¹æ®å‚æ•°ç¡®å®šæ•°æ®é›†ç±»å‹å’Œå¾®è°ƒé…ç½®
        3. åŠ è½½ Stable Diffusion 3 æ¨¡å‹ï¼ˆå¯é€‰åŠ è½½ LoRA æƒé‡ï¼‰
        4. åŠ è½½ Faster R-CNN ç›®æ ‡æ£€æµ‹æ¨¡å‹
        5. ä»æ•°æ®é›†åŠ è½½å›¾åƒæè¿°å’Œåœºæ™¯-å¯¹è±¡æ˜ å°„
        6. å¯¹æ¯ä¸ªå›¾åƒï¼š
           a. éšæœºé€‰æ‹©ä¸€ä¸ªæè¿°
           b. æ ¹æ®åœºæ™¯è·å–å¯èƒ½çš„å¯¹è±¡åˆ—è¡¨
           c. ä½¿ç”¨ GPT æ¨¡å‹å¢å¼ºæè¿°ï¼Œæ·»åŠ å¯¹è±¡ä¿¡æ¯
           d. ä½¿ç”¨ SD3 ç”Ÿæˆå›¾åƒ
           e. ä½¿ç”¨ Faster R-CNN æ£€æµ‹å›¾åƒä¸­çš„å¯¹è±¡
           f. ä¿å­˜å›¾åƒã€æ ‡æ³¨æ–‡ä»¶å’Œå¢å¼ºåçš„æè¿°
    """
    import openai
    
    # ========== åˆå§‹åŒ– OpenAI API ==========
    # å®šä¹‰ä¸åŒæ¨¡å‹çš„ API åŸºç¡€ URL
    api_base = {
                "gpt-3.5-turbo": "https://api.openai.com/v1",
                "gpt-3.5": "https://api.openai.com/v1",
                "gpt-4.1": "https://api.openai.com/v1",
                "gpt-5": "https://api.openai.com/v1",
                }   
    
    # ========== ç¡®å®šæ•°æ®é›†ç±»å‹å’Œå¾®è°ƒé…ç½® ==========
    ft = ''  # å¾®è°ƒæ ‡è¯†å­—ç¬¦ä¸²
    
    # æ ¹æ®å¾®è°ƒæ•°æ®åç§°è‡ªåŠ¨ç¡®å®šæ•°æ®é›†ç±»å‹
    if 'coco' in args.ftdata:
        args.dataset = 'coco'
    else:
        args.dataset = 'voc'
        
    # å¦‚æœæŒ‡å®šäº†å¾®è°ƒæ­¥æ•°ï¼Œæ„å»ºå¾®è°ƒæ ‡è¯†å­—ç¬¦ä¸²
    if args.ftstep > 0:
        ft += '{}-ftstep-{}'.format(args.ftdata, args.ftstep)
    
    # ========== è®¾ç½® LLM æ¨¡å‹ ==========
    llmmodel = 'gpt-4.1'  # ä½¿ç”¨ GPT-4.1 è¿›è¡Œæç¤ºè¯å¢å¼º
    args.llmmodel = llmmodel
    openai.api_base = api_base[args.llmmodel]
    # åˆ›å»º OpenAI å®¢æˆ·ç«¯
    client = openai.OpenAI(base_url='https://api.openai.com/v1')

    # ========== æ„å»ºè¾“å‡ºè·¯å¾„ ==========
    cap_model = ''  # æè¿°æ¨¡å‹æ ‡è¯†ï¼ˆå½“å‰ä¸ºç©ºï¼‰
    
    # æ„å»ºè¾“å‡ºç›®å½•è·¯å¾„ï¼ŒåŒ…å«æ‰€æœ‰é…ç½®ä¿¡æ¯
    submsg = './results/sd-release/gen_04/stable-diffusion-3-medium-diffusers/{}{}/{}/{}/maxcap-{}-variou-tc'.format(
                args.dataset, cap_model, ft, llmmodel, args.maxcap)
        
    # æ·»åŠ æ¨ç†æ­¥æ•°ä¿¡æ¯
    submsg += '-inferstep-{}'.format(args.inferstep)
    # å¦‚æœæŒ‡å®šäº†ç‰¹æ®Šç±»å‹ï¼Œæ·»åŠ ç±»å‹å’Œæœ€å¤§å¯¹è±¡æ•°ä¿¡æ¯
    if args.type != 'default':
        submsg += '-{}-{}'.format(args.type, args.maxobj)
    
    # å®šä¹‰å„ç§è¾“å‡ºç›®å½•
    output_img_dir = os.path.join(submsg, 'images/train2017')  # ç”Ÿæˆçš„å›¾åƒç›®å½•
    label_gen_path = os.path.join(submsg, 'labels_gen/faster-rcnn/train2017')  # æ£€æµ‹æ ‡æ³¨ç›®å½•
    caption_path = os.path.join(submsg, 'caption_merge/')  # å¢å¼ºåçš„æè¿°ç›®å½•
    
    # åˆ›å»ºæ‰€æœ‰å¿…è¦çš„è¾“å‡ºç›®å½•
    for f in [output_img_dir, label_gen_path, caption_path]:
        os.makedirs(f, exist_ok=True)

    # ========== åŠ è½½ Stable Diffusion 3 æ¨¡å‹ ==========
    print('Load SD')
    # ä» HuggingFace åŠ è½½ SD3 Medium æ¨¡å‹ï¼Œä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°ä»¥èŠ‚çœæ˜¾å­˜
    pipe = StableDiffusion3Pipeline.from_pretrained("stabilityai/stable-diffusion-3-medium-diffusers", torch_dtype=torch.float16)
    print('Load SD done')
    # å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU
    # ä¿®æ”¹cudaä¸ºcpu/mpsï¼ˆMac M1 Pro ä½¿ç”¨ mpsï¼‰
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    pipe = pipe.to(device)
    # å¯ç”¨ CPU å¸è½½ä»¥èŠ‚çœæ˜¾å­˜ï¼ˆå°†ä¸ä½¿ç”¨çš„æ¨¡å‹ç»„ä»¶ç§»åˆ° CPUï¼‰
    # æ³¨æ„ï¼šMPS æ¨¡å¼ä¸‹å¯èƒ½ä¸éœ€è¦æˆ–æ”¯æŒ CPU å¸è½½ï¼Œå¦‚æœå‡ºé”™å¯ä»¥æ³¨é‡Šæ‰
    if device == "cpu":
    pipe.enable_model_cpu_offload()
    pipe = pipe.to(device)
    
    # ========== åŠ è½½ LoRA å¾®è°ƒæƒé‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰ ==========
    if args.ftstep > 0:
        if args.dataset == 'voc':
            if args.ftdata == 'voc10k':
                # åŠ è½½ VOC 10k æ•°æ®é›†å¾®è°ƒçš„ LoRA æƒé‡
                pipe.load_lora_weights('./results/ftvoc/pytorch_lora_weights.safetensors')
            elif args.ftdata == 'voc10kmerge':
                # åŠ è½½ VOC 10k åˆå¹¶æ•°æ®é›†çš„ LoRA æƒé‡
                pipe.load_lora_weights('xxx/pytorch_lora_weights.safetensors')
            else:
                raise ValueError(f"Unknown ftdata for VOC: {args.ftdata}")
        elif args.dataset == 'coco':
            if args.ftdata == 'coco20k':
                # åŠ è½½ COCO 20k æ•°æ®é›†å¾®è°ƒçš„ LoRA æƒé‡
                pipe.load_lora_weights('xx')
            elif args.ftdata == 'coco20kmerge':
                # åŠ è½½ COCO 20k åˆå¹¶æ•°æ®é›†çš„ LoRA æƒé‡
                pipe.load_lora_weights('xs')
            else:
                raise ValueError(f"Unknown ftdata for COCO: {args.ftdata}")

    print('Here')

    # ========== åŠ è½½ç›®æ ‡æ£€æµ‹æ¨¡å‹ ==========
    print('Init Detector')
    # ä¿®æ”¹cudaä¸ºcpu/mpsï¼ˆMac M1 Pro ä½¿ç”¨ mpsï¼Œä½† MMDetection å¯èƒ½ä¸æ”¯æŒ mpsï¼Œä½¿ç”¨ cpuï¼‰
    # æ³¨æ„ï¼šMMDetection åœ¨ Mac ä¸Šé€šå¸¸åªæ”¯æŒ CPUï¼ŒMPS æ”¯æŒå¯èƒ½ä¸å®Œæ•´
    detector_device = "cpu"  # MMDetection åœ¨ Mac ä¸Šå»ºè®®ä½¿ç”¨ CPU
    if args.dataset == 'voc':
        # VOC æ•°æ®é›†çš„ Faster R-CNN é…ç½®å’Œæƒé‡
        config_file = './configs/pascal_voc/faster-rcnn_r50_fpn_1x_voc0712.py'
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ° dataset/voc/ è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ results/ è·¯å¾„
        if os.path.exists('./dataset/voc/pretrain/faster-rcnn/faster_rcnn_r50_fpn_1x_voc0712.pth'):
            checkpoint_file = './dataset/voc/pretrain/faster-rcnn/faster_rcnn_r50_fpn_1x_voc0712.pth'
        else:
        checkpoint_file = './results/pretrain/voc/faster-rcnn/faster_rcnn_r50_fpn_1x_voc0712.pth'
        # model = init_detector(config_file, checkpoint_file, device='cuda:0')  # åŸ CUDA ä»£ç 
        model = init_detector(config_file, checkpoint_file, device=detector_device)
    elif args.dataset == 'coco':
        # COCO æ•°æ®é›†çš„ Faster R-CNN é…ç½®å’Œæƒé‡
        config_file = 'configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py'
        checkpoint_file = './results/pretrain/coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'
        # model = init_detector(config_file, checkpoint_file, device='cuda:0')  # åŸ CUDA ä»£ç 
        model = init_detector(config_file, checkpoint_file, device=detector_device)

    print('Init Detector Done')
    
    # æ•°æ®é›†åç§°æ˜ å°„ï¼ˆç”¨äºæç¤ºè¯ï¼‰
    d = {'coco': 'COCO', 'voc': 'VOC'}

    # ========== åŠ è½½æ•°æ®é›†æè¿°å’Œåœºæ™¯-å¯¹è±¡æ˜ å°„ ==========
    all_captions, scene2obj = get_cap()
    
    # ========== ä¸»å¾ªç¯ï¼šç”Ÿæˆå›¾åƒå¹¶æ£€æµ‹ ==========
    for i in range(args.begin, args.end):
        # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
        random.seed(i)
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        fn = '{}-GEN.jpg'.format(i+1)
        
        # ä»æ‰€æœ‰æè¿°ä¸­éšæœºé€‰æ‹©ä¸€ä¸ª
        selected = random.sample(all_captions, 1)
        # æå–æè¿°æ–‡æœ¬
        captions = [s['text'] for s in selected]
        print(captions)

        # ========== è·å–å¯èƒ½çš„å¯¹è±¡åˆ—è¡¨ ==========
        # æ ¹æ®é€‰ä¸­å›¾åƒçš„åœºæ™¯ï¼Œè·å–è¯¥åœºæ™¯ä¸­å¯èƒ½å‡ºç°çš„æ‰€æœ‰å¯¹è±¡
        possible_obj = [scene2obj[_['scene']] for _ in selected]
        # å°†åµŒå¥—åˆ—è¡¨å±•å¹³
        possible_obj = list(chain.from_iterable(possible_obj))
        # å¦‚æœå¯¹è±¡å¤ªå¤šï¼Œéšæœºé€‰æ‹©10ä¸ªï¼ˆé¿å…æç¤ºè¯è¿‡é•¿ï¼‰
        if len(possible_obj) > 10:
            possible_obj = random.sample(possible_obj, 10)
        # å°†å¯¹è±¡åˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
        possible_obj = ", ".join(possible_obj)
        
        # ========== æ„å»º LLM æç¤ºè¯ï¼ˆ4è±¡é™ç‰ˆæœ¬ï¼‰ ==========
        # ç³»ç»Ÿæç¤ºè¯ï¼šå®šä¹‰ GPT çš„è§’è‰²å’Œä»»åŠ¡ï¼Œè¦æ±‚ç”Ÿæˆ4ä¸ªè±¡é™çš„æè¿°
        system_msg = (
            "You are a caption enhancement assistant specialized in generating object-rich image prompts for Stable Diffusion, "
            f"with a focus on datasets like {d[args.dataset]}. Your task is to create a 4-quadrant image composition by enriching a given caption "
            "and incorporating a provided list of possible objects. "
            "You must generate exactly 4 separate captions, one for each quadrant (Top Left, Top Right, Bottom Left, Bottom Right). "
            "Each caption should describe a coherent scene that fits naturally in its quadrant position. "
            "Distribute the objects from the provided list across the 4 quadrants in a logical and visually appealing way. "
            "Each caption should be a single, fluent sentence (ideally under 30 words), suitable for Stable Diffusion image generation. "
            "Format your response exactly as follows:\n"
            "Caption:\n"
            "Top Left: [description]\n"
            "Top Right: [description]\n"
            "Bottom Left: [description]\n"
            "Bottom Right: [description]"
            )

        # æ„å»ºç”¨æˆ·æç¤ºè¯ï¼šåŒ…å«åŸå§‹æè¿°å’Œå¯èƒ½çš„å¯¹è±¡åˆ—è¡¨
        caption_blocks = [f"Caption {i+1}:\n{captions[i]}\n" for i in range(1)]
        caption_text = "\n".join(caption_blocks)
        
        user_prompt = (
            f"Original Caption:\n{caption_text}\n\n"
            f"Here is a list of possible objects to consider: {possible_obj}. "
            f"Please create 4 quadrant captions by distributing these objects across Top Left, Top Right, Bottom Left, and Bottom Right quadrants. "
            "Each quadrant should have a coherent scene description that naturally incorporates some of the objects from the list. "
            "The 4 quadrants should work together to form a complete, visually interesting composition."
        )

        # ========== ä½¿ç”¨ GPT ç”Ÿæˆ4è±¡é™æè¿° ==========
        rewrite_caption = get_llm_output(client, system_msg, user_prompt, args.llmmodel)
        print("åŸå§‹æè¿°:", captions)
        print("å¯èƒ½å¯¹è±¡:", possible_obj)
        print("LLMç”Ÿæˆçš„4è±¡é™æè¿°:\n", rewrite_caption)
        
        # ========== è§£æ4è±¡é™æè¿° ==========
        quadrant_prompts = parse_quadrant_captions(rewrite_caption)
        print("è§£æåçš„è±¡é™æè¿°:")
        for key, prompt in quadrant_prompts.items():
            print(f"  {key}: {prompt}")
        
        # ========== ä¸ºæ¯ä¸ªè±¡é™ç”Ÿæˆå›¾åƒ ==========
        print("æ­£åœ¨ç”Ÿæˆ4ä¸ªè±¡é™çš„å›¾åƒ...")
        top_left_img = gen_quadrant_image(pipe, quadrant_prompts['top_left'], args.inferstep, seed_offset=0)
        top_right_img = gen_quadrant_image(pipe, quadrant_prompts['top_right'], args.inferstep, seed_offset=1)
        bottom_left_img = gen_quadrant_image(pipe, quadrant_prompts['bottom_left'], args.inferstep, seed_offset=2)
        bottom_right_img = gen_quadrant_image(pipe, quadrant_prompts['bottom_right'], args.inferstep, seed_offset=3)
        
        # ========== æ‹¼æ¥4ä¸ªè±¡é™æˆå®Œæ•´å›¾åƒ ==========
        print("æ­£åœ¨æ‹¼æ¥4ä¸ªè±¡é™...")
        combined_image = combine_quadrants(top_left_img, top_right_img, bottom_left_img, bottom_right_img)
        
        # ========== ä¿å­˜æ‹¼æ¥åçš„å®Œæ•´å›¾åƒ ==========
        combined_image.save(join(output_img_dir, fn))
        print(f"å·²ä¿å­˜å®Œæ•´å›¾åƒ: {fn}")
        
        # ========== å¯¹æ‹¼æ¥åçš„å®Œæ•´å›¾åƒè¿›è¡Œç›®æ ‡æ£€æµ‹ ==========
        print("æ­£åœ¨è¿›è¡Œç›®æ ‡æ£€æµ‹...")
        results = inference_detector(model, join(output_img_dir, fn))
        # ä¿å­˜æ£€æµ‹ç»“æœï¼ˆå¯è§†åŒ–å›¾åƒå’Œæ ‡æ³¨æ–‡ä»¶ï¼‰
        save_predict(join(output_img_dir, fn), results, '{}'.format(i+1), output_img_dir, label_gen_path)
        
        # ========== ä¿å­˜å¢å¼ºåçš„æè¿°ï¼ˆåŒ…å«4è±¡é™æè¿°ï¼‰ ==========
        with open(join(caption_path, '{}.txt'.format(i+1)), 'w') as f:
            f.write("Caption:\n")
            f.write(f"Top Left: {quadrant_prompts['top_left']}\n")
            f.write(f"Top Right: {quadrant_prompts['top_right']}\n")
            f.write(f"Bottom Left: {quadrant_prompts['bottom_left']}\n")
            f.write(f"Bottom Right: {quadrant_prompts['bottom_right']}\n")


# ========== å‘½ä»¤è¡Œå‚æ•°è§£æ ==========
parser = argparse.ArgumentParser(description='Stable Diffusion 3 å›¾åƒç”Ÿæˆä¸ç›®æ ‡æ£€æµ‹æµæ°´çº¿')

# åŸºæœ¬å‚æ•°
parser.add_argument("--maxcap", type=int, default=4, help="æœ€å¤§æè¿°æ•°é‡")
parser.add_argument("--begin", type=int, default=0, help="å¼€å§‹ç”Ÿæˆçš„å›¾åƒç´¢å¼•")
parser.add_argument("--end", type=int, default=1, help="ç»“æŸç”Ÿæˆçš„å›¾åƒç´¢å¼•ï¼ˆä¸åŒ…å«ï¼‰")
parser.add_argument("--randomsample", type=int, default=50, help="éšæœºé‡‡æ ·æ•°é‡")
parser.add_argument("--llmmodel", type=str, default='gpt-3.5-turbo', help="LLM æ¨¡å‹åç§°ï¼ˆgpt-3.5-turbo, gpt-4.1 ç­‰ï¼‰")
parser.add_argument("--topk", type=int, default=5, help="Top-K é‡‡æ ·å‚æ•°")
parser.add_argument("--cluster", type=int, default=200, help="èšç±»æ•°é‡")
parser.add_argument("--rebalance", type=str2bool, default=False, help="æ˜¯å¦é‡æ–°å¹³è¡¡æ•°æ®")
parser.add_argument("--drop_prob", type=float, default=0.5, help="ä¸¢å¼ƒæ¦‚ç‡")
parser.add_argument("--maxobj", type=int, default=2, help="æœ€å¤§å¯¹è±¡æ•°é‡")
parser.add_argument("--inferstep", type=int, default=50, help="SD3 æ¨ç†æ­¥æ•°ï¼ˆè¶Šå¤šè´¨é‡è¶Šå¥½ä½†é€Ÿåº¦è¶Šæ…¢ï¼‰")
parser.add_argument("--ftstep", type=int, default=0, help="å¾®è°ƒæ­¥æ•°ï¼ˆ0è¡¨ç¤ºä¸ä½¿ç”¨å¾®è°ƒï¼‰")
parser.add_argument("--pretrainstep", type=int, default=1000, help="é¢„è®­ç»ƒæ­¥æ•°")
parser.add_argument("--capaug", type=str2bool, default=False, help="æ˜¯å¦ä½¿ç”¨æè¿°å¢å¼º")
parser.add_argument("--dataset", type=str, default='coco', help="æ•°æ®é›†ç±»å‹ï¼ˆcoco æˆ– vocï¼‰")

# å¾®è°ƒç›¸å…³å‚æ•°
parser.add_argument("--ftdata", type=str, default='voc10k', help="å¾®è°ƒæ•°æ®åç§°ï¼ˆvoc10k, coco20k ç­‰ï¼‰")
parser.add_argument("--ncluster", type=int, default=128, help="èšç±»æ•°é‡")

# é«˜çº§å‚æ•°
parser.add_argument("--alpha", type=float, default=1.0, help="Alpha å‚æ•°")
parser.add_argument("--beta", type=float, default=1.0, help="Beta å‚æ•°")
parser.add_argument("--sampled", type=int, default=0, help="é‡‡æ ·æ•°é‡")
parser.add_argument("--method", type=str, default='topk', help="é‡‡æ ·æ–¹æ³•")
parser.add_argument("--k", type=int, default=0, help="K å€¼å‚æ•°")
parser.add_argument("--portion", type=float, default=1.0, help="æ•°æ®æ¯”ä¾‹")
parser.add_argument("--lam", type=float, default=1.0, help="Lambda å‚æ•°")
parser.add_argument("--enrich", type=str2bool, default=False, help="æ˜¯å¦å¯ç”¨å¢å¼º")
parser.add_argument("--partition", type=str2bool, default=False, help="æ˜¯å¦åˆ†åŒº")
parser.add_argument("--cross", type=str2bool, default=False, help="æ˜¯å¦äº¤å‰éªŒè¯")
parser.add_argument("--sdft", type=str, default='coco', help="SD å¾®è°ƒæ•°æ®é›†")
parser.add_argument("--type", type=str, default='default', help="ç”Ÿæˆç±»å‹")
parser.add_argument("--botk", type=int, default=10, help="Bottom-K å‚æ•°")

# æ¨¡å‹ç›¸å…³å‚æ•°
parser.add_argument("--sd15", type=str2bool, default=False, help="æ˜¯å¦ä½¿ç”¨ SD 1.5ï¼ˆå½“å‰ä½¿ç”¨ SD3ï¼‰")
parser.add_argument("--lorarank", type=int, default=12, help="LoRA ç§©ï¼ˆrankï¼‰")

# è§£æå‚æ•°
args = parser.parse_args()

# ========== æ‰§è¡Œä¸»å‡½æ•° ==========
gen_04()


'''
# ç¤ºä¾‹ä»£ç ï¼ˆå·²æ³¨é‡Šï¼‰
# from diffusers import StableDiffusion3Pipeline
# import torch
# import time
# pipe = StableDiffusion3Pipeline.from_pretrained("stabilityai/stable-diffusion-3-medium-diffusers", torch_dtype=torch.float16)
# # ä¿®æ”¹cudaä¸ºcpu/mpsï¼ˆMac M1 Pro ä½¿ç”¨ mpsï¼‰
# device = "mps" if torch.backends.mps.is_available() else "cpu"
# pipe = pipe.to(device)
# if device == "cpu":
#     pipe.enable_model_cpu_offload()
# 
# # ä¿®æ”¹cudaä¸ºcpu/mpsï¼ˆMac M1 Pro ä½¿ç”¨ mpsï¼‰
# generator = torch.Generator(device).manual_seed(int(time.time() * 1000) % 100000)

prompt='Busy city street with 6 persons, 3 cars, 1 bus, 2 bicycles. Person left of bicycle; car in front of bus; person partially occluded by car. Small and large instances, street-level, mid-shot. Crosswalk and storefronts in background. Overcast daylight, motion blur on legs; one person cropped at right edge'

out = pipe(prompt,num_inference_steps=50, generator=generator)

out.images[0].save('a.jpg')


prompt='A modern Christian poster design featuring a peaceful sunrise or sunset sky with soft pastel colors, rays of light shining through gentle clouds. A large white cross on the top-right side. Minimalist, inspirational, divine atmosphere, digital art, high resolution.'

prompt = 'A modern Christian poster with a serene, inspirational feel. The background features a beautiful pastel-colored sky during sunrise, soft golden sunlight breaking through fluffy clouds, creating radiant beams of light from the horizon. On the right side, a clean white cross glows subtly with a slight shadow for depth. On the left, bold, elegant Vietnamese text in a modern sans-serif font reads: "Äá»¨C CHÃšA TRá»œI ÄÃƒ SAI CON Má»˜T NGÃ€I Äáº¾N THáº¾-GIAN, Äáº¶NG CHÃšNG TA NHá»œ CON ÄÆ¯á»¢C Sá»NG". The text is navy blue with smooth edges and balanced spacing. The overall design is minimalist yet reverent, spiritual and uplifting, in a digital art style, ultra high resolution, perfect lighting, no clutter.'


prompt = "A modern Christian digital poster radiating hope and divine purpose. The background features a vivid, majestic sunrise over an open landscape â€” brilliant golden light bursting through layers of luminous, multi-toned clouds in the sky. Ethereal rays extend toward the horizon, symbolizing a bright future and spiritual awakening. In the distance, a glowing city of light or celestial architecture can be subtly seen, representing the Kingdom of God or divine destiny. Soft rolling hills and flowering fields gently lead the eye toward a glowing white cross on the right foreground, standing firm on a sunlit hill. Light particles shimmer in the air like holy embers. The entire scene is illuminated with warm, heavenly light, conveying clarity, purpose, and future hope. The composition is highly inspirational, spiritual, futuristic, and visually striking. Digital painting style, ultra high resolution, cinematic lighting, divine realism, 4K."

prompt='A powerful, spiritually symbolic Christian poster in a modern digital art style. The background shows a radiant, heavenly sky at dawn â€” brilliant golden light pouring through parted clouds, forming rays that stretch across the scene, representing divine presence. High above, a descending dove glows softly, symbolizing the Holy Spirit. In the center-right, a large white cross stands firmly on a grassy hill, bathed in holy light. At its base, a small, open Bible rests on a stone, its pages glowing with divine illumination. In the distance, the faint outline of the New Jerusalem â€” golden towers and shining gates â€” emerges from the horizon, suggesting the eternal promise of salvation. A narrow, glowing path leads from the viewerâ€™s perspective toward the cross and onward to the heavenly city, symbolizing the journey of faith. Small, ethereal beams of light rise from the earth, representing prayers and the souls being guided. The overall design is layered, deeply symbolic, visionary, filled with hope and spiritual depth. Ultra high resolution, cinematic lighting, divine realism, perfect for digital posters.'



'''